---

---






# Model Card for BeansTeam 2CL+2FL

<!-- Provide a quick summary of what the model is/does. [Optional] -->
It consists of a concatenation of two convolutional layers (2D) and two fully connected (FC) layers, right before a LogSoftmax activation over a 3 neuron output layer that ends with the classification to one of the classes depending on the maximum value over those neurons




#  Table of Contents

- [Model Card for BeansTeam 2CL+2FL](#model-card-for--model_id-)
- [Table of Contents](#table-of-contents)
- [Table of Contents](#table-of-contents-1)
- [Model Details](#model-details)
  - [Model Description](#model-description)
- [Uses](#uses)
  - [Direct Use](#direct-use)
  - [Downstream Use [Optional]](#downstream-use-optional)
  - [Out-of-Scope Use](#out-of-scope-use)
- [Bias, Risks, and Limitations](#bias-risks-and-limitations)
  - [Recommendations](#recommendations)
- [Training Details](#training-details)
  - [Training Data](#training-data)
  - [Training Procedure](#training-procedure)
    - [Preprocessing](#preprocessing)
    - [Speeds, Sizes, Times](#speeds-sizes-times)
- [Evaluation](#evaluation)
  - [Testing Data, Factors & Metrics](#testing-data-factors--metrics)
    - [Testing Data](#testing-data)
    - [Factors](#factors)
    - [Metrics](#metrics)
  - [Results](#results)
- [Model Examination](#model-examination)
- [Environmental Impact](#environmental-impact)
- [Technical Specifications [optional]](#technical-specifications-optional)
  - [Model Architecture and Objective](#model-architecture-and-objective)
  - [Compute Infrastructure](#compute-infrastructure)
    - [Hardware](#hardware)
    - [Software](#software)
- [Citation](#citation)
- [Glossary [optional]](#glossary-optional)
- [More Information [optional]](#more-information-optional)
- [Model Card Authors [optional]](#model-card-authors-optional)
- [Model Card Contact](#model-card-contact)
- [How to Get Started with the Model](#how-to-get-started-with-the-model)


# Model Details

## Model Description

<!-- Provide a longer summary of what this model is/does. -->
It consists of a concatenation of two convolutional layers (2D) and two fully connected (FC) layers, right before a LogSoftmax activation over a 3 neuron output layer that ends with the classification to one of the classes depending on the maximum value over those neurons

- **Developed by:** More information needed
- **Shared by [Optional]:** More information needed
- **Model type:** Language model
- **Language(s) (NLP):** en, es
- **License:** mit
- **Parent Model:** More information needed
- **Resources for more information:** More information needed
    - [GitHub Repo](https://github.com/MLOps-essi-upc/MLOps-TeamBeans)


# Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

## Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->
<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say "more info needed." -->




## Downstream Use [Optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->
<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say "more info needed." -->
 



## Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->
<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say "more info needed." -->

Our model is mainly focused in the upper part of single leaf classification, with the allowance of a hand holding that leaf, as many instances of our training also contain that. Nonetheless, multiple leaf instances are well classified if applicable, with the condition that the image fed has to be sized at 500x500. Images of whole plants or photographs of the bottom of the leaf are out-of-scope for our model. Low light images are also out of the scope of the model training data, and shouldnâ€™t be considered for a reliable estimator of leaf conditions by the model.



# Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.


## Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->





# Training Details

## Training Data

<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

More information on training data needed


## Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

### Preprocessing

More information needed

### Speeds, Sizes, Times

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

More information needed
 
# Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

## Testing Data, Factors & Metrics

### Testing Data

<!-- This should link to a Data Card if possible. -->

The data used for this classification model is called &#34;beans&#34; and is readily available from Hugging Face. It comes pre-divided into training, test, and validation subsets, making it convenient for model development and evaluation. This dataset comprises 1295 images of 500x500 RGB bean leaves, classified into three categories: angular leaf spot, bean rust, and healthy.


### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

More information needed

### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

1. Micro F1-Score: A comprehensive measure of model performance that considers precision and recall, weighted by class frequencies. Appropriate for balanced datasets, such as ours.

2. Weighted Disease Recall: An indicator of the model&#39;s ability to correctly identify disease cases, calculated as the average of recall values for the disease classes. Weight of 62% for bean rust and 38% for angular leaf spot, based on dangerousness for the plant.


## Results 

79% micro F1, 82% weighted recall

# Model Examination

More information needed

# Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** GPU
- **Hours used:** More information needed
- **Cloud Provider:** AWS
- **Compute Region:** More information needed
- **Carbon Emitted:** 0.000618 kWh

# Technical Specifications [optional]

## Model Architecture and Objective

concatenation of two convolutional layers (2D) and two fully connected (FC) layers, right before a LogSoftmax activation over a 3 neuron output layer that ends with the classification to one of the classes depending on the maximum value over those neurons. Chosen loss for the training is Negative Log-Likelihood

## Compute Infrastructure

More information needed

### Hardware

GPU

### Software

PyTorch

# Citation

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

More information needed

**APA:**

More information needed

# Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

More information needed

# More Information [optional]

More information needed

# Model Card Authors [optional]

<!-- This section provides another layer of transparency and accountability. Whose views is this model card representing? How many voices were included in its construction? Etc. -->

P, a, u,  , C, o, m, a, s, ,,  , W, o, j, c, i, e, c, h,  , W, o, j, z, s, t, e, k, ,,  , M, a, t, h, e, u, s, z,  , G, a, l, i, n, s, k, i, ,,  , M, i, o, n, a,  , D, i, m, i, c, ,,  , J, o, a, n,  , O, l, i, v, e, r, a, s

# Model Card Contact

More information needed

# How to Get Started with the Model

Use the code below to get started with the model.

<details>
<summary> Click to expand </summary>

More information needed

</details>